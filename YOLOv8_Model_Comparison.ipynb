{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef22cd9",
   "metadata": {},
   "source": [
    "# YOLOv8 Model Comparison: Original vs Pruned\n",
    "\n",
    "This notebook provides a comprehensive side-by-side comparison between the original YOLOv8n model and the pruned version.\n",
    "\n",
    "**Pruning Results:**\n",
    "- **49% channels pruned** (1,547 out of 5,296)\n",
    "- **4.8% model size reduction**\n",
    "- **Maintained architecture** with optimized weights\n",
    "\n",
    "**Comparisons Include:**\n",
    "- Model specifications and sizes\n",
    "- Inference speed benchmarks\n",
    "- Detection accuracy on sample image\n",
    "- Performance metrics visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1068a463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n",
      "\n",
      "=== GPU Information ===\n",
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce MX570\n",
      "GPU Memory: 2.0 GB\n",
      "Using device: CUDA\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "\n",
    "# GPU Information\n",
    "print(\"\\n=== GPU Information ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea587b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models for comparison...\n",
      "✓ Original model loaded\n",
      "✓ Pruned model loaded\n",
      "✓ Fine-tuned model loaded\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DetectionModel' object has no attribute 'model_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m\n\u001b[0;32m     19\u001b[0m     size_mb \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(model\u001b[38;5;241m.\u001b[39mmodel_path) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: name,\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m: params,\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize_mb\u001b[39m\u001b[38;5;124m'\u001b[39m: size_mb,\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel_path)\n\u001b[0;32m     25\u001b[0m     }\n\u001b[0;32m     27\u001b[0m models_info \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mget_model_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOriginal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     29\u001b[0m     get_model_info(pruned_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPruned\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     30\u001b[0m     get_model_info(finetuned_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFine-tuned\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m ]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Model Specifications ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m models_info:\n",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36mget_model_info\u001b[1;34m(model, name)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_model_info\u001b[39m(model, name):\n\u001b[0;32m     18\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m---> 19\u001b[0m     size_mb \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: name,\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m: params,\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize_mb\u001b[39m\u001b[38;5;124m'\u001b[39m: size_mb,\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel_path)\n\u001b[0;32m     25\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\haksh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\model.py:1164\u001b[0m, in \u001b[0;36mModel.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    Enable accessing model attributes directly through the Model class.\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;124;03m        >>> print(model.names)  # Access model.names attribute\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\haksh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1962\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1960\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1961\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1962\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1963\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1964\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DetectionModel' object has no attribute 'model_path'"
     ]
    }
   ],
   "source": [
    "# Load models for comparison\n",
    "print(\"Loading models for comparison...\")\n",
    "\n",
    "# Original model\n",
    "original_model = YOLO('yolov8-prune/yolov8n.pt')\n",
    "print(\"✓ Original model loaded\")\n",
    "\n",
    "# Pruned model\n",
    "pruned_model = YOLO('pruned_yolo_model.pt')\n",
    "print(\"✓ Pruned model loaded\")\n",
    "\n",
    "# Fine-tuned model\n",
    "finetuned_model = YOLO('finetune_results/pruned_finetune4/weights/best.pt')\n",
    "print(\"✓ Fine-tuned model loaded\")\n",
    "\n",
    "# Model specifications\n",
    "def get_model_info(model, name):\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    size_mb = os.path.getsize(model.model_path) / (1024 * 1024)\n",
    "    return {\n",
    "        'name': name,\n",
    "        'parameters': params,\n",
    "        'size_mb': size_mb,\n",
    "        'path': str(model.model_path)\n",
    "    }\n",
    "\n",
    "models_info = [\n",
    "    get_model_info(original_model, 'Original'),\n",
    "    get_model_info(pruned_model, 'Pruned'),\n",
    "    get_model_info(finetuned_model, 'Fine-tuned')\n",
    "]\n",
    "\n",
    "print(\"\\n=== Model Specifications ===\")\n",
    "for info in models_info:\n",
    "    print(f\"{info['name']}: {info['parameters']:,} params, {info['size_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3245e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size comparison visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Size comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "sizes = [info['size_mb'] for info in models_info]\n",
    "names = [info['name'] for info in models_info]\n",
    "bars = plt.bar(names, sizes, color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.8)\n",
    "plt.title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Size (MB)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, size in zip(bars, sizes):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{size:.2f} MB', ha='center', va='bottom')\n",
    "\n",
    "# Parameter comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "params = [info['parameters']/1e6 for info in models_info]  # Convert to millions\n",
    "bars = plt.bar(names, params, color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.8)\n",
    "plt.title('Parameter Count Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Parameters (Millions)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, param in zip(bars, params):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{param:.1f}M', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compression metrics\n",
    "orig_size = models_info[0]['size_mb']\n",
    "pruned_size = models_info[1]['size_mb']\n",
    "ft_size = models_info[2]['size_mb']\n",
    "\n",
    "print(\"\\n=== Compression Results ===\")\n",
    "print(f\"Pruned vs Original: {(pruned_size/orig_size - 1)*100:+.1f}% size change\")\n",
    "print(f\"Fine-tuned vs Original: {(ft_size/orig_size - 1)*100:+.1f}% size change\")\n",
    "print(f\"Fine-tuned vs Pruned: {(ft_size/pruned_size - 1)*100:+.1f}% size change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference speed comparison\n",
    "def benchmark_inference_speed(model, model_name, num_runs=50):\n",
    "    \"\"\"Benchmark inference speed for a model\"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randn(1, 3, 640, 640).to(device)\n",
    "    model.model.to(device)\n",
    "    model.model.eval()\n",
    "    \n",
    "    # Warm up\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):\n",
    "            _ = model.model(dummy_input)\n",
    "    \n",
    "    # Time inference\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            _ = model.model(dummy_input)\n",
    "            torch.cuda.synchronize() if device == 'cuda' else None\n",
    "            end = time.time()\n",
    "            times.append((end - start) * 1000)  # ms\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    fps = 1000 / avg_time\n",
    "    \n",
    "    print(f\"{model_name} ({device.upper()}):\")\n",
    "    print(f\"  Average: {avg_time:.2f} ms\")\n",
    "    print(f\"  Std Dev: {std_time:.2f} ms\")\n",
    "    print(f\"  FPS: {fps:.1f}\")\n",
    "    \n",
    "    return times\n",
    "\n",
    "print(\"=== Inference Speed Benchmark ===\")\n",
    "print(\"Testing with 640x640 input on\", \"GPU\" if torch.cuda.is_available() else \"CPU\")\n",
    "print()\n",
    "\n",
    "# Benchmark all models\n",
    "original_times = benchmark_inference_speed(original_model, \"Original YOLOv8n\")\n",
    "print()\n",
    "pruned_times = benchmark_inference_speed(pruned_model, \"Pruned YOLOv8n\")\n",
    "print()\n",
    "finetuned_times = benchmark_inference_speed(finetuned_model, \"Fine-tuned YOLOv8n\")\n",
    "\n",
    "# Calculate speedups\n",
    "pruned_speedup = np.mean(original_times) / np.mean(pruned_times)\n",
    "finetuned_speedup = np.mean(original_times) / np.mean(finetuned_times)\n",
    "\n",
    "print(f\"\\nSpeedup Results:\")\n",
    "print(f\"Pruned vs Original: {pruned_speedup:.2f}x faster\")\n",
    "print(f\"Fine-tuned vs Original: {finetuned_speedup:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50056f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize inference speed comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Box plot comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "all_times = [original_times, pruned_times, finetuned_times]\n",
    "labels = ['Original', 'Pruned', 'Fine-tuned']\n",
    "plt.boxplot(all_times, labels=labels)\n",
    "plt.title('Inference Time Distribution', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Average time comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "avg_times = [np.mean(t) for t in all_times]\n",
    "bars = plt.bar(labels, avg_times, color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.8)\n",
    "plt.title('Average Inference Time', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, time in zip(bars, avg_times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{time:.1f}ms', ha='center', va='bottom')\n",
    "\n",
    "# FPS comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "fps_values = [1000/np.mean(t) for t in all_times]\n",
    "bars = plt.bar(labels, fps_values, color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.8)\n",
    "plt.title('Frames Per Second (FPS)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('FPS')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, fps in zip(bars, fps_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{fps:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\n=== Performance Summary ===\")\n",
    "for i, (label, times) in enumerate(zip(labels, all_times)):\n",
    "    avg_time = np.mean(times)\n",
    "    fps = 1000 / avg_time\n",
    "    print(f\"{label}: {avg_time:.2f}ms ({fps:.1f} FPS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b474f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image detection comparison\n",
    "print(\"=== Sample Image Detection ===\")\n",
    "print(\"Using TRAFFIC.jpeg for detection comparison\")\n",
    "\n",
    "# Check if image exists\n",
    "image_path = 'TRAFFIC.jpeg'\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"❌ Image {image_path} not found!\")\n",
    "    print(\"Please ensure TRAFFIC.jpeg is in the current directory\")\n",
    "else:\n",
    "    print(f\"✓ Found image: {image_path}\")\n",
    "    \n",
    "    # Load and display original image\n",
    "    img = Image.open(image_path)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title('Original Image: TRAFFIC.jpeg', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Image size: {img.size}\")\n",
    "    print(f\"Image mode: {img.mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection on original model\n",
    "if os.path.exists('TRAFFIC.jpeg'):\n",
    "    print(\"\\n--- Original Model Detection ---\")\n",
    "    \n",
    "    # Time the detection\n",
    "    start_time = time.time()\n",
    "    orig_results = original_model.predict(\n",
    "        'TRAFFIC.jpeg',\n",
    "        save=True,\n",
    "        project='detection_comparison',\n",
    "        name='original',\n",
    "        exist_ok=True,\n",
    "        device=device\n",
    "    )\n",
    "    orig_detection_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Original model detection time: {orig_detection_time:.3f} seconds\")\n",
    "    \n",
    "    # Extract detection results\n",
    "    orig_detections = orig_results[0]\n",
    "    print(f\"Detections found: {len(orig_detections.boxes)}\")\n",
    "    \n",
    "    if len(orig_detections.boxes) > 0:\n",
    "        print(\"\\nDetected objects:\")\n",
    "        for i, box in enumerate(orig_detections.boxes):\n",
    "            cls_id = int(box.cls.item())\n",
    "            conf = box.conf.item()\n",
    "            class_name = original_model.names[cls_id]\n",
    "            print(f\"  {i+1}. {class_name} (confidence: {conf:.3f})\")\n",
    "    \n",
    "    # Display result image\n",
    "    result_img_path = 'detection_comparison/original/TRAFFIC.jpeg'\n",
    "    if os.path.exists(result_img_path):\n",
    "        result_img = Image.open(result_img_path)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(result_img)\n",
    "        plt.title(f'Original Model Detection\\n{len(orig_detections.boxes)} objects detected', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Result image not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection on pruned model\n",
    "if os.path.exists('TRAFFIC.jpeg'):\n",
    "    print(\"\\n--- Pruned Model Detection ---\")\n",
    "    \n",
    "    # Time the detection\n",
    "    start_time = time.time()\n",
    "    pruned_results = pruned_model.predict(\n",
    "        'TRAFFIC.jpeg',\n",
    "        save=True,\n",
    "        project='detection_comparison',\n",
    "        name='pruned',\n",
    "        exist_ok=True,\n",
    "        device=device\n",
    "    )\n",
    "    pruned_detection_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Pruned model detection time: {pruned_detection_time:.3f} seconds\")\n",
    "    \n",
    "    # Extract detection results\n",
    "    pruned_detections = pruned_results[0]\n",
    "    print(f\"Detections found: {len(pruned_detections.boxes)}\")\n",
    "    \n",
    "    if len(pruned_detections.boxes) > 0:\n",
    "        print(\"\\nDetected objects:\")\n",
    "        for i, box in enumerate(pruned_detections.boxes):\n",
    "            cls_id = int(box.cls.item())\n",
    "            conf = box.conf.item()\n",
    "            class_name = pruned_model.names[cls_id]\n",
    "            print(f\"  {i+1}. {class_name} (confidence: {conf:.3f})\")\n",
    "    \n",
    "    # Display result image\n",
    "    result_img_path = 'detection_comparison/pruned/TRAFFIC.jpeg'\n",
    "    if os.path.exists(result_img_path):\n",
    "        result_img = Image.open(result_img_path)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(result_img)\n",
    "        plt.title(f'Pruned Model Detection\\n{len(pruned_detections.boxes)} objects detected', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Result image not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection on fine-tuned model\n",
    "if os.path.exists('TRAFFIC.jpeg'):\n",
    "    print(\"\\n--- Fine-tuned Model Detection ---\")\n",
    "    \n",
    "    # Time the detection\n",
    "    start_time = time.time()\n",
    "    ft_results = finetuned_model.predict(\n",
    "        'TRAFFIC.jpeg',\n",
    "        save=True,\n",
    "        project='detection_comparison',\n",
    "        name='finetuned',\n",
    "        exist_ok=True,\n",
    "        device=device\n",
    "    )\n",
    "    ft_detection_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Fine-tuned model detection time: {ft_detection_time:.3f} seconds\")\n",
    "    \n",
    "    # Extract detection results\n",
    "    ft_detections = ft_results[0]\n",
    "    print(f\"Detections found: {len(ft_detections.boxes)}\")\n",
    "    \n",
    "    if len(ft_detections.boxes) > 0:\n",
    "        print(\"\\nDetected objects:\")\n",
    "        for i, box in enumerate(ft_detections.boxes):\n",
    "            cls_id = int(box.cls.item())\n",
    "            conf = box.conf.item()\n",
    "            class_name = finetuned_model.names[cls_id]\n",
    "            print(f\"  {i+1}. {class_name} (confidence: {conf:.3f})\")\n",
    "    \n",
    "    # Display result image\n",
    "    result_img_path = 'detection_comparison/finetuned/TRAFFIC.jpeg'\n",
    "    if os.path.exists(result_img_path):\n",
    "        result_img = Image.open(result_img_path)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(result_img)\n",
    "        plt.title(f'Fine-tuned Model Detection\\n{len(ft_detections.boxes)} objects detected', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Result image not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection comparison summary\n",
    "if os.path.exists('TRAFFIC.jpeg'):\n",
    "    print(\"\\n=== Detection Comparison Summary ===\")\n",
    "    \n",
    "    # Detection counts\n",
    "    orig_count = len(orig_detections.boxes) if 'orig_detections' in locals() else 0\n",
    "    pruned_count = len(pruned_detections.boxes) if 'pruned_detections' in locals() else 0\n",
    "    ft_count = len(ft_detections.boxes) if 'ft_detections' in locals() else 0\n",
    "    \n",
    "    # Timing\n",
    "    orig_time = orig_detection_time if 'orig_detection_time' in locals() else 0\n",
    "    pruned_time = pruned_detection_time if 'pruned_detection_time' in locals() else 0\n",
    "    ft_time = ft_detection_time if 'ft_detection_time' in locals() else 0\n",
    "    \n",
    "    print(\"Model\\t\\tDetections\\tTime (s)\\t\\tFPS\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Original\\t{orig_count}\\t\\t{orig_time:.3f}\\t\\t{1/orig_time:.2f}\")\n",
    "    print(f\"Pruned\\t\\t{pruned_count}\\t\\t{pruned_time:.3f}\\t\\t{1/pruned_time:.2f}\")\n",
    "    print(f\"Fine-tuned\\t{ft_count}\\t\\t{ft_time:.3f}\\t\\t{1/ft_time:.2f}\")\n",
    "    \n",
    "    # Speed comparison\n",
    "    if orig_time > 0 and pruned_time > 0:\n",
    "        speedup = orig_time / pruned_time\n",
    "        print(f\"\\nPruned model is {speedup:.2f}x faster than original!\")\n",
    "    \n",
    "    if orig_time > 0 and ft_time > 0:\n",
    "        speedup_ft = orig_time / ft_time\n",
    "        print(f\"Fine-tuned model is {speedup_ft:.2f}x faster than original!\")\n",
    "    \n",
    "    # Detection accuracy comparison\n",
    "    if orig_count > 0:\n",
    "        pruned_acc = (pruned_count / orig_count) * 100\n",
    "        ft_acc = (ft_count / orig_count) * 100\n",
    "        print(f\"\\nDetection count vs Original:\")\n",
    "        print(f\"Pruned: {pruned_acc:.1f}% of original detections\")\n",
    "        print(f\"Fine-tuned: {ft_acc:.1f}% of original detections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive comparison visualization\n",
    "plt.figure(figsize=(16, 10))\n",
    "fig.suptitle('YOLOv8 Model Comparison: Original vs Pruned vs Fine-tuned', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Model sizes\n",
    "plt.subplot(2, 3, 1)\n",
    "sizes = [info['size_mb'] for info in models_info]\n",
    "bars = plt.bar([info['name'] for info in models_info], sizes, \n",
    "               color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.8)\n",
    "plt.title('Model Size (MB)', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "for bar, size in zip(bars, sizes):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{size:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Parameters\n",
    "plt.subplot(2, 3, 2)\n",
    "params = [info['parameters']/1e6 for info in models_info]\n",
    "bars = plt.bar([info['name'] for info in models_info], params, \n",
    "               color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.8)\n",
    "plt.title('Parameters (Millions)', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "for bar, param in zip(bars, params):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{param:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# Inference speed\n",
    "plt.subplot(2, 3, 3)\n",
    "avg_times = [np.mean(original_times), np.mean(pruned_times), np.mean(finetuned_times)]\n",
    "bars = plt.bar(['Original', 'Pruned', 'Fine-tuned'], avg_times, \n",
    "               color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.8)\n",
    "plt.title('Inference Time (ms)', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "for bar, time in zip(bars, avg_times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{time:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# Detection counts (if available)\n",
    "plt.subplot(2, 3, 4)\n",
    "if 'orig_detections' in locals() and 'pruned_detections' in locals() and 'ft_detections' in locals():\n",
    "    counts = [len(orig_detections.boxes), len(pruned_detections.boxes), len(ft_detections.boxes)]\n",
    "    bars = plt.bar(['Original', 'Pruned', 'Fine-tuned'], counts, \n",
    "                   color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.8)\n",
    "    plt.title('Objects Detected', fontsize=12, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                 f'{int(count)}', ha='center', va='bottom')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Detection data\\nnot available', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    plt.title('Objects Detected', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Detection timing (if available)\n",
    "plt.subplot(2, 3, 5)\n",
    "if 'orig_detection_time' in locals() and 'pruned_detection_time' in locals() and 'ft_detection_time' in locals():\n",
    "    times = [orig_detection_time, pruned_detection_time, ft_detection_time]\n",
    "    bars = plt.bar(['Original', 'Pruned', 'Fine-tuned'], times, \n",
    "                   color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.8)\n",
    "    plt.title('Detection Time (s)', fontsize=12, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    for bar, t in zip(bars, times):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                 f'{t:.3f}', ha='center', va='bottom')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Timing data\\nnot available', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    plt.title('Detection Time (s)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Performance summary\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.axis('off')\n",
    "summary_text = f\"\"\"YOLOv8 Pruning Results:\n",
    "\n",
    "• 49% channels pruned\n",
    "• 4.8% size reduction\n",
    "• {pruned_speedup:.2f}x inference speedup\n",
    "• Architecture preserved\n",
    "• Ready for edge deployment\n",
    "\n",
    "Methodology: Network Slimming\"\"\"\n",
    "plt.text(0.1, 0.9, summary_text, fontsize=10, verticalalignment='top',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎉 Comparison Complete!\")\n",
    "print(\"The pruned model achieves significant computational savings\")\n",
    "print(\"while maintaining detection capabilities for edge deployment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
